# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J7bxCUtpGHCEwgT1_1m_lRL3c0rIo2t4
"""


import os
import torch
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint

import wandb
from torch import optim, nn, utils, Tensor

import pytorch_lightning as pl
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
from pytorch_lightning.loggers import WandbLogger
import glob
import pandas as pd





def make_water_data(raw, data_length, drop_columns):
    df_raw=[]
    for domain in raw:
        df=pd.read_excel("water_data.xlsx", domain + "_samples").drop_duplicates()
        #Add time
        df['day'] = df.DateTime.dt.day
        df['month'] = df.DateTime.dt.month
        df['year'] = df.DateTime.dt.year
        month_col = ["day", 'month']

        #drop things
        qual_col = [col for col in df.columns if 'Qualifiers' in col]
        for col in qual_col:
            df = df[df[col].isna()].drop(col, axis=1)
        df = df.drop(['DateTime', 'Value [COND] Conductivity (umho)'], axis=1)
        df = df.dropna()

        #one hot encode date
        for col in month_col:
            df = pd.concat((df, pd.get_dummies(df[col], prefix=[col])), axis=1).drop(col, axis=1)

        #normalize years
        year_idx = df.columns.get_loc("year")
        df_year = df.iloc[:, year_idx]
        df.iloc[:, year_idx] = (df_year - df_year.min()) / (df_year.max() - df_year.min())
        df_raw.append(df)

    #get normalization info
    all_domains=pd.concat(df_raw,axis=0)
    #year_idx from above
    df_norm = all_domains.iloc[:, :year_idx]
    ########################################################
    result=[]

    #variables in main


    for domain in df_raw:
        #normalize
        domain.iloc[:, :year_idx] = (df_norm - df_norm.mean()) / df_norm.std()

        #add missing column
        data_col = domain.columns[:year_idx]
        for col in data_col:
            domain.insert(domain.columns.get_loc(col) + 1
                      , col + " missing", 0)
        masked, plain = [], []

        for i in range(0, len(domain) - window_length, window_distance):
            data_entry = domain.iloc[i:i + window_length].copy()
            plain.append(torch.tensor(data_entry.values).float())

            long_miss_time = np.random.randint(0, window_length - len_long_miss, size=num_long_miss)
            short_miss_time = np.random.randint(0, window_length - len_short_miss, size=num_short_miss)
            col_nums = np.random.choice([k for k in range(9)], size=7, replace=False)
            for j, col_num in enumerate(col_nums):
                if j < num_long_miss:
                    data_entry.iloc[long_miss_time[j]: long_miss_time[j] + len_long_miss, col_num * 2] = 0
                    data_entry.iloc[long_miss_time[j]: long_miss_time[j] + len_long_miss, col_num * 2 + 1] = 1
                else:
                    data_entry.iloc[short_miss_time[j - num_long_miss]: short_miss_time[j - num_long_miss] + len_short_miss,
                    col_num * 2] = 0
                    data_entry.iloc[short_miss_time[j - num_long_miss]: short_miss_time[j - num_long_miss] + len_short_miss,
                    col_num * 2 + 1] = 1
            masked.append(torch.tensor(data_entry.values).float())

        result.append(torch.stack(masked))
        result.append(torch.stack(plain))
    return result


    # result=[]
    # for i in range(num_domains):
    #     result.append(x[i:i+1, :, :])
    #     result.append(y[i:i+1, :, :])
    # #[(num_points, in_dim), num_points(out_dim)]
    # return result


class LinearRelu(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, depth):
        super().__init__()
        self.net = nn.ModuleList([nn.Linear(in_dim, hidden_dim), nn.ReLU()])
        for i in range(depth):
            self.net.append(nn.Linear(hidden_dim, hidden_dim))
            self.net.append(nn.ReLU())

        self.net.append(nn.Linear(hidden_dim, out_dim))
        self.net=nn.Sequential(*self.net)
    def forward(self, x):
        return self.net(x)
class LinearElu(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, depth):
        super().__init__()
        self.net = nn.ModuleList([nn.Linear(in_dim, hidden_dim), nn.ELU()])
        for i in range(depth):
            self.net.append(nn.Linear(hidden_dim, hidden_dim))
            self.net.append(nn.ELU())

        self.net.append(nn.Linear(hidden_dim, out_dim))
        self.net=nn.Sequential(*self.net)
    def forward(self, x):
        return self.net(x)
class Linear(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, depth):
        super().__init__()
        self.net = nn.ModuleList([nn.Linear(in_dim, hidden_dim)])
        for i in range(depth):
            self.net.append(nn.Linear(hidden_dim, hidden_dim))

        self.net.append(nn.Linear(hidden_dim, out_dim))
        self.net=nn.Sequential(*self.net)
    def forward(self, x):
        return self.net(x)

class SequenceModule(nn.Module):
    def __init__(self, in_dim, seq_hidden_dim, init_hidden_dim, out_dim, warmup_length):
        super().__init__()
        self.num_layers=2
        self.seq_hidden_dim=seq_hidden_dim
        self.in_dim=in_dim
        self.out_dim=out_dim
        self.warmup_length=warmup_length

        self.lstm=nn.LSTM(in_dim, hidden_size=seq_hidden_dim, num_layers=self.num_layers, proj_size=out_dim, batch_first=True)
        self.lstm_warmup=nn.LSTM(in_dim+out_dim,
                                 hidden_size=init_hidden_dim,
                                 proj_size=out_dim,
                                 num_layers=self.num_layers,
                                 batch_first=True
                                 )
    def forward(self, x):
        #batch, length, dim
        _, (h0, c0) =self.lstm_warmup(x[:, :self.warmup_length])
        return self.lstm(x[:,self.warmup_length:,:self.in_dim], (h0, c0))

class ImputeModel(nn.Module):
    def __init__(self, in_dim, seq_hidden_dim, out_dim):
        super().__init__()
        self.num_layers=2
        self.seq_hidden_dim=seq_hidden_dim
        self.in_dim=in_dim
        self.out_dim=out_dim

        self.lstm=nn.LSTM(in_dim, hidden_size=seq_hidden_dim, num_layers=self.num_layers,
                          proj_size=out_dim, bidirectional=True,batch_first=True)
        self.reproject = nn.Linear(2*out_dim, out_dim)
    def forward(self, x):
        #batch, length, dim
        return self.reproject(self.lstm(x)[0])


class TrainInvariant(pl.LightningModule):
    def __init__(self, in_dim, f_embed_dim,  g_embed_dim, out_dim, num_domains):
        super().__init__()
        self.in_dim=in_dim
        self.warmup_length=warmup_length
        # self.invariant=SequenceModule(in_dim, f_embed_dim, f_embed_dim, out_dim, warmup_length)
        # self.train_variants=nn.ModuleList([SequenceModule(in_dim, g_embed_dim, g_embed_dim, out_dim, warmup_length) for i in range (num_domains)])
        # self.test_variant=SequenceModule(in_dim, g_embed_dim, g_embed_dim, out_dim, warmup_length)

        self.invariant = ImputeModel(in_dim,f_embed_dim, out_dim)
        self.train_variants = nn.ModuleList(
            [ImputeModel(in_dim, g_embed_dim, out_dim) for i in range(num_domains)])
        self.test_variant = ImputeModel(in_dim, g_embed_dim, out_dim)
        self.num_domains=num_domains
        self.eta=lambda x,y: x+y
        self.num_quantities=9
        self.loss=nn.MSELoss(reduction="sum")
    def forward(self, x, domain_num):
        result_f = self.invariant(x)
        result_g =self.train_variants[domain_num](x)

        return self.eta(result_f[0], result_g[0])

    def training_step(self, batch, batch_idx):
        # training_step defines the train loop.
        # it is independent of forward
        total_loss=0
        # lin_loss=0
        for i in range(0,self.num_domains*2, 2):
            #batch, length, dim
            quantities=batch[i+1][:,:, :2*self.num_quantities][:,:, ::2]
            mask=batch[i][:,:, :2*self.num_quantities][:,:, 1::2]

            total_loss+=self.loss(self(batch[i], i)*mask, quantities*mask)/mask.sum()
            # lin_interpolated=pd.DataFrame(quantities)
            # lin_interpolated[mask]=pd.NA
            # lin_interpolated=torch.tensor(lin_interpolated.interpolate().values).cuda()
            # lin_loss+=self.loss(lin_interpolated,quantities)
            # total_loss+=self.loss(self(batch[i], i), quantities)/quantities.numel()
        self.log("train_loss", total_loss/self.num_domains)
        # self.log("linear interp", lin_loss/self.num_domains)
        return total_loss/self.num_domains

    def validation_step(self, batch, batch_idx) :
        total_loss = 0
        lin_loss=0
        for i in range(0, self.num_domains * 2, 2):
            # batch, length, dim
            quantities = batch[i + 1][:, :, :2 * self.num_quantities][:, :, ::2]
            mask = batch[i][:, :, :2 * self.num_quantities][:, :, 1::2]
            total_loss += self.loss(self(batch[i], i) * mask, quantities * mask) /mask.sum()
            lin_interpolated = pd.DataFrame(numpy_it(quantities.squeeze()))
            lin_interpolated[numpy_it(mask.squeeze().bool())] = pd.NA
            lin_interpolated = torch.tensor(lin_interpolated.interpolate().values).cuda()
            lin_loss += self.loss(lin_interpolated, quantities)/mask.sum()
        self.log("val_loss", total_loss / self.num_domains)
        # wandb.log("linear_interp", lin_loss/self.num_domains)

        wandb.log({"linear_interp": lin_loss/self.num_domains})
    # def on_fit_end(self):

    def configure_optimizers(self):
        #check if all there
        optimizer = optim.Adam(self.parameters(), lr=1e-3)

        return optimizer



def get_latest_file():
    list_of_files = glob.glob(os.path.dirname(__file__) + '/DA Thesis/**/*.ckpt',
                              recursive=True)  # * means all if need specific format then *.csv
    latest_file = max(list_of_files, key=os.path.getctime)
    return latest_file
def numpy_it(t):
    return t.detach().cpu().numpy()
if __name__=="__main__":
    # define any number of nn.Modules (or use your current ones)
    drop_columns=["Unnamed: 0", "time2.1.nsteps.",
                  "rain_t_2"]

    all_data = []
    path = "/content/drive/MyDrive/Masters/Thesis/water_data.xlsx"
    raw = [
        "Maumee",
        # "Portage",
        # "Raisin",
        # "Rock Creek",
        # "S. Turkeyfoot",
        # "Sandusky River",
        # "Wolf",

    ]

    np.random.seed(0)
    torch.use_deterministic_algorithms(True)
    torch.manual_seed(0)

    key="c20d41ecf28a9b0efa2c5acb361828d1319bc62e"


    predict_length = 200
    warmup_length=100
    data_length= predict_length + warmup_length
    f_embed_dim = 500
    g_embed_dim=50
    max_epoch=100
    window_length = 152
    len_long_miss, len_short_miss = 10, 2
    num_long_miss, num_short_miss = 2, 5
    total_miss=len_long_miss*num_long_miss+len_short_miss*num_short_miss
    window_distance = window_length // 3

    all_dataset = TensorDataset(*make_water_data(raw=raw, data_length=data_length, drop_columns=drop_columns))
    in_dim = 62
    out_dim = 9
    num_domains=len(raw)
    split_fraction=0.7
    split_point=int(len(all_dataset)*split_fraction)

    #no random
    train_dataset = all_dataset[:split_point]
    val_dataset = all_dataset[:split_point]

    #random
    train_dataset, val_dataset = torch.utils.data.random_split(all_dataset, (split_fraction,1-split_fraction))

    # train_dataset = TensorDataset(*make_quad_data_val(num_val_io_pairs, val_coef))
    # val_dataset = TensorDataset(*make_quad_data_val(num_points, val_coef))

    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)
    val_loader = DataLoader(val_dataset,batch_size=1)
    # val_loader=train_loader
    wandb_logger=WandbLogger(project="DA Thesis", name="trash",log_model="True")
    wandb.init() ########################################################
    wandb_logger.experiment.config.update({
                                           "num_domains": num_domains,
                                           "f_embed_dim":f_embed_dim,
                                           "g_embed_dim": g_embed_dim,
                                           "max_epoch": max_epoch,
                                           "file":os.path.basename(__file__)})

    # base_trans=TrainInvariant(in_dim, f_embed_dim=f_embed_dim, g_embed_dim=g_embed_dim,out_dim=out_dim,num_domains=num_domains)
    #

    # base_trans = TrainInvariant.load_from_checkpoint(get_latest_file(),in_dim=in_dim, f_embed_dim=f_embed_dim,
    #                                                  g_embed_dim=g_embed_dim,
    #                                                  out_dim=out_dim,
    #                                                  num_domains=num_domains)
    base_trans = TrainInvariant.load_from_checkpoint("epoch=99-step=10000.ckpt",
                                                     in_dim=in_dim,
                                                     f_embed_dim=f_embed_dim,
                                                     g_embed_dim=g_embed_dim,
                                                     out_dim=out_dim,
                                                     num_domains=num_domains,
                                                     )


    wandb_logger.watch(base_trans, log="all")

    early_stop_callback = EarlyStopping(monitor="val_loss", min_delta=0, patience=100)
    # small_error_callback = EarlyStopping(monitor="val_loss", stopping_threshold=0.02)
    model_callback = ModelCheckpoint(
        save_top_k=1,
        monitor="val_loss",
        mode="min",

    )
    trainer1 = pl.Trainer(limit_train_batches=100,
                          logger=wandb_logger,
                          max_epochs=max_epoch,
                          # log_every_n_steps = 5,
                          accelerator="gpu", devices=1,
                          callbacks=[
                              model_callback,
                              early_stop_callback,
                                     # small_error_callback
                                     # model_checkpoint
                                     ],
                      # fast_dev_run=True
                      )



    #
    # idea_module

    # trainer1.fit(idea_module2, train_loader,val_loader)

    trainer1.fit(base_trans, train_loader, val_loader)

    wandb.save(get_latest_file())

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /content/drive/MyDrive/Masters/Thesis/lightning_logs --port=8008
#
# val_dataset[6]
#
# train_dataset[0]
#
# idea_module2(val_dataset[0:1][0],val_dataset[0:1][2] )

# val_dataset[0:1][1]



# Commented out IPython magic to ensure Python compatibility.
# %debug

# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J7bxCUtpGHCEwgT1_1m_lRL3c0rIo2t4
"""