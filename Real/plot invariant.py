# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J7bxCUtpGHCEwgT1_1m_lRL3c0rIo2t4
"""
import copy
import os
import torch
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint

import wandb
from torch import optim, nn, utils, Tensor
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor
import pytorch_lightning as pl
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
from pytorch_lightning import loggers as pl_loggers
from pytorch_lightning.loggers import WandbLogger
import glob
import matplotlib.pyplot as plt
import pandas as pd
import datetime




def make_water_data(raw, data_length, drop_columns):
    df_raw=[]
    for domain in raw:
        df=pd.read_excel("water_data.xlsx", domain + "_samples").drop_duplicates()

        qual_col = [col for col in df.columns if 'Qualifiers' in col]
        df = df.drop('Value [COND] Conductivity (umho)', axis=1)

        for col in qual_col:
            df = df[df[col].isna()].drop(col, axis=1)
        df["abs_time"] = (df.DateTime - datetime.datetime(2000, 1, 1)).dt.total_seconds() / 60
        df = df.dropna()

        cyclic_day = (df.DateTime.dt.minute + df.DateTime.dt.hour * 60) / 1440 *2*np.pi
        df['cyclic_day_sin'] = np.sin(cyclic_day)
        df['cyclic_day_cos'] = np.cos(cyclic_day)

        cyclic_year = df.DateTime.dt.dayofyear/ 365*2*np.pi
        df['cyclic_year_sin'] = np.sin(cyclic_year)
        df['cyclic_year_cos'] = np.cos(cyclic_year)

        df = df.drop('DateTime', axis=1)
        year_idx = df.columns.get_loc("abs_time")



        df_raw.append(df)

    #get normalization info
    all_domains=pd.concat(df_raw,axis=0)
    #year_idx from above
    df_features = all_domains.iloc[:, :year_idx]
    df_abs_time=all_domains.iloc[:, year_idx]

    ########################################################
    result=[]

    #variables in main


    for domain in df_raw:
        #normalize
        domain.iloc[:, :year_idx] = (domain.iloc[:, :year_idx] - df_features.mean()) / df_features.std()
        domain.iloc[:,year_idx]=(domain.iloc[:,year_idx]-df_abs_time.min())/(df_abs_time.max()-df_abs_time.min())


        result.append(torch.tensor(domain.values).float())



    return result, df



class LinearRelu(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, depth):
        super().__init__()
        self.net = nn.ModuleList([nn.Linear(in_dim, hidden_dim), nn.ReLU()])
        for i in range(depth):
            self.net.append(nn.Linear(hidden_dim, hidden_dim))
            self.net.append(nn.ReLU())

        self.net.append(nn.Linear(hidden_dim, out_dim))
        self.net=nn.Sequential(*self.net)
    def forward(self, x):
        return self.net(x)
class LinearElu(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, depth):
        super().__init__()
        self.net = nn.ModuleList([nn.Linear(in_dim, hidden_dim), nn.ELU()])
        for i in range(depth):
            self.net.append(nn.Linear(hidden_dim, hidden_dim))
            self.net.append(nn.ELU())

        self.net.append(nn.Linear(hidden_dim, out_dim))
        self.net=nn.Sequential(*self.net)
    def forward(self, x):
        return self.net(x)
class Linear(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, depth):
        super().__init__()
        self.net = nn.ModuleList([nn.Linear(in_dim, hidden_dim)])
        for i in range(depth):
            self.net.append(nn.Linear(hidden_dim, hidden_dim))

        self.net.append(nn.Linear(hidden_dim, out_dim))
        self.net=nn.Sequential(*self.net)
    def forward(self, x):
        return self.net(x)

class SequenceModule(nn.Module):
    def __init__(self, in_dim, seq_hidden_dim, init_hidden_dim, out_dim, warmup_length):
        super().__init__()
        self.num_layers=2
        self.seq_hidden_dim=seq_hidden_dim
        self.in_dim=in_dim
        self.out_dim=out_dim
        self.warmup_length=warmup_length

        self.lstm=nn.LSTM(in_dim, hidden_size=seq_hidden_dim, num_layers=self.num_layers, proj_size=out_dim, batch_first=True)
        self.lstm_warmup=nn.LSTM(in_dim+out_dim,
                                 hidden_size=init_hidden_dim,
                                 proj_size=out_dim,
                                 num_layers=self.num_layers,
                                 batch_first=True
                                 )
    def forward(self, x):
        #batch, length, dim
        _, (h0, c0) =self.lstm_warmup(x[:, :self.warmup_length])
        return self.lstm(x[:,self.warmup_length:,:self.in_dim], (h0, c0))


class TrainInvariant(pl.LightningModule):
    def __init__(self, in_dim, f_embed_dim,  g_embed_dim, out_dim, num_domains, warmup_length):
        super().__init__()
        self.in_dim=in_dim
        self.warmup_length=warmup_length
        self.invariant=SequenceModule(in_dim, f_embed_dim, f_embed_dim, out_dim, warmup_length)
        self.train_variants=nn.ModuleList([SequenceModule(in_dim, g_embed_dim, g_embed_dim, out_dim, warmup_length) for i in range (num_domains)])
        self.test_variant=SequenceModule(in_dim, g_embed_dim, g_embed_dim, out_dim, warmup_length)
        self.num_domains=num_domains
        self.eta=lambda x,y: x+y

        self.loss=nn.MSELoss()
    def forward(self, x, domain_num):
        result_f = self.invariant(x)
        result_g =self.train_variants[domain_num](x)

        return self.eta(result_f[0], result_g[0])

    def training_step(self, batch, batch_idx):
        # training_step defines the train loop.
        # it is independent of forward

        losses=[self.loss(self(batch[i], i), batch[i][:, self.warmup_length:,self.in_dim:]) for i in range (self.num_domains)]
        loss=0
        for i in range(self.num_domains):
            loss+=losses[i]
        loss=loss/len(losses)
        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx) :
        losses = [self.loss(self(batch[i], i), batch[i][:, self.warmup_length:,self.in_dim:]) for i in range(self.num_domains)]
        loss = 0
        for i in range(self.num_domains):
            loss += losses[i]
        loss = loss / len(losses)        # Logging to TensorBoard (if installed) by default
        self.log("val_loss", loss)
    # def on_fit_end(self):

    def configure_optimizers(self):
        #check if all there
        optimizer = optim.Adam(self.parameters(), lr=1e-3)

        return optimizer



def get_latest_file():
    list_of_files = glob.glob(os.path.dirname(__file__) + '/DA Thesis/**/*.ckpt',
                              recursive=True)  # * means all if need specific format then *.csv
    latest_file = max(list_of_files, key=os.path.getctime)
    return latest_file
def numpy_it(t):
    return t.detach().cpu().numpy()
if __name__=="__main__":
    # define any number of nn.Modules (or use your current ones)
    drop_columns=["Unnamed: 0", "time2.1.nsteps.",
                  "rain_t_2"]
    all_data=[]
    # for file in os.listdir("data"):
    #     one_domain = pd.read_csv("data/"+file)
    #     one_domain=one_domain.drop("Unnamed: 0",1)
    #     all_data[file]=one_domain

    raw = [
        "Maumee",
        # "Portage",
        # "Raisin",
        # "Rock Creek",
        # "S. Turkeyfoot",
        # "Sandusky River",
        # "Wolf",

    ]

    np.random.seed(0)
    torch.use_deterministic_algorithms(True)
    torch.manual_seed(0)

    key="c20d41ecf28a9b0efa2c5acb361828d1319bc62e"


    predict_length = 102
    warmup_length=50
    window_length= predict_length + warmup_length
    f_embed_dim = 100
    g_embed_dim=50
    # f_layers=2
    # g_layers=2
    max_epoch=3

    window_distance = window_length // 5
    # window length / drop columnsnot used
    in_dim = 5
    out_dim = 9
    num_domains = len(raw)





    base_trans = TrainInvariant.load_from_checkpoint("epoch=1938-step=21329.ckpt",
                                                    in_dim=in_dim,
                                                     f_embed_dim=f_embed_dim,
                                                     g_embed_dim=g_embed_dim,
                                                     out_dim=out_dim,
                                                     num_domains=num_domains,
                                                     warmup_length=warmup_length)



    correct, df=make_water_data(raw, window_length, drop_columns)
    correct[0]=correct[0][:1000]
    result=copy.deepcopy(correct)
    result2=copy.deepcopy(correct)

    for i in range (len(correct)):
        for j in range(0, result[i].shape[0]-window_length, predict_length):
            result[i][warmup_length+j: window_length+j, :out_dim]=\
                base_trans(correct[i][j:j+window_length].reshape((1, window_length, -1)), i)
            # result[i][100+j*200: 300+j*200, 1:]=base_trans.invariant(correct[i][j*200:j*200+300].reshape((1, 300, -1)))[0]
            # result2[i][100+j*200: 300+j*200, 1:]=base_trans.train_variants[i](correct[i][j*200:j*200+300].reshape((1, 300, -1)))[0]

    # for i in range (len(correct)):
    #     result[i][100: 7300, 1:]=base_trans(correct[i].reshape((1, 7300, -1)), i)
    for i in range(len(correct)):
        for j in range(
                 correct[i].shape[1]
        ):
            plt.title(f"{i}")
            plt.plot(correct[i][:, j].cpu().detach(), label = df.columns[j]+"df")
            plt.plot(result[i][:,j].cpu().detach(), label = df.columns[j]+"predict")
            # plt.plot(result[i][:,j].cpu().detach(), label = df.columns[j]+"predictf")
            # plt.plot(result2[i][:,j].cpu().detach(), label = df.columns[j]+"predictg")

            plt.legend()
            plt.show()
