# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J7bxCUtpGHCEwgT1_1m_lRL3c0rIo2t4
"""


import os
import torch
from pytorch_lightning.callbacks import EarlyStopping

import wandb
from torch import optim, nn, utils, Tensor
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor
import pytorch_lightning as pl
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
from pytorch_lightning import loggers as pl_loggers
from pytorch_lightning.loggers import WandbLogger
import glob
from torch.optim import Optimizer
from torch.optim.lr_scheduler import ReduceLROnPlateau

def make_quad_data(num_domains, num_points, coef):
#make double range
    x=np.random.rand(num_domains, num_points, in_dim)
    x_transpose=x.transpose(2, 1, 0)
    #coef num_domains, powers
    y=np.array([coef[:, i] * x_transpose ** i
                for i in range(coef.shape[1])]).sum(axis=0)\
        .transpose(2, 1, 0)



    return x,y
def make_quad_data_val(num_points, coef):
    x=np.random.rand(num_points, in_dim)
    y=np.array([(coef[i]*x**i) for i in range(coef.shape[1])]).sum(axis=0)
    #y will be num_points, out_dim=1

    return x,y


def make_weird_data(num_domains, num_points, coef):
    x=np.random.rand(num_domains, num_points, in_dim)
    x_transpose=x.transpose(2, 1, 0)
    y=(coef[:,0] * np.e ** x_transpose + \
      coef[:,1] * x_transpose ** 2 + \
      coef[:,2] * np.sin(10 * x_transpose))\
        .transpose(2, 1, 0)
    #coef will be num_domain, 3
    #y will be num_domains, num_points, out_dim=1
    x=torch.Tensor(x)
    y=torch.Tensor(y)
    result=[]
    for i in range(num_domains):
        result.append(x[i, :, :])
        result.append(y[i, :, :])
    #[(num_points, in_dim), num_points(out_dim)]
    return result
def make_weird_data_val(num_points, coef):
    x=np.random.rand(num_points, in_dim)

    y=(coef[0]*np.e**x+ \
      coef[1] *x**2 + \
      coef[2]*np.sin(10*x)).sum(axis=0)\

    #coef will be 3
    #y will be num_points, out_dim=1
    x=torch.Tensor(x)
    y=torch.Tensor(y)
    return x,y
class linear_relu(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, depth):
        super().__init__()
        self.net = nn.ModuleList([nn.Linear(in_dim, hidden_dim), nn.ReLU()])
        for i in range(depth):
            self.net.append(nn.Linear(hidden_dim, hidden_dim))
            self.net.append(nn.ReLU())

        self.net.append(nn.Linear(hidden_dim, out_dim))
        self.net=nn.Sequential(*self.net)
    def forward(self, x):
        return self.net(x)
class linear_elu(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, depth):
        super().__init__()
        self.net = nn.ModuleList([nn.Linear(in_dim, hidden_dim), nn.ELU()])
        for i in range(depth):
            self.net.append(nn.Linear(hidden_dim, hidden_dim))
            self.net.append(nn.ELU())

        self.net.append(nn.Linear(hidden_dim, out_dim))
        self.net=nn.Sequential(*self.net)
    def forward(self, x):
        return self.net(x)
class linear(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, depth):
        super().__init__()
        self.net = nn.ModuleList([nn.Linear(in_dim, hidden_dim)])
        for i in range(depth):
            self.net.append(nn.Linear(hidden_dim, hidden_dim))

        self.net.append(nn.Linear(hidden_dim, out_dim))
        self.net=nn.Sequential(*self.net)
    def forward(self, x):
        return self.net(x)




def get_latest_file():
    list_of_files = glob.glob(os.path.dirname(__file__) + '/DA Thesis/**/*.ckpt',
                              recursive=True)  # * means all if need specific format then *.csv
    latest_file = max(list_of_files, key=os.path.getctime)
    return latest_file
if __name__=="__main__":
    # define any number of nn.Modules (or use your current ones)


    in_dim = 1
    out_dim = 1
    np.random.seed(0)
    torch.use_deterministic_algorithms(True)
    torch.manual_seed(0)

    f_embed_dim = 6
    g_embed_dim = 2
    num_domains = 2
    gen_deg=4

    train_coef = np.array([[1.26, -18.95, 73.5, -105,50,10,4,9,10]])

    # train_coef = np.concatenate((np.random.rand(num_domains,g_embed_dim+1),
    #                              np.random.normal(loc=1, scale=0.1, size=(num_domains, gen_deg-g_embed_dim))), axis=1)
    # train_coef = np.random.normal(loc=1, scale=0.2, size=(num_domains, 3))

    num_domains = train_coef.shape[0]

    num_points = 100
    max_epoch=3000

    train_dataset = make_quad_data(num_domains, num_points,train_coef)
    val_dataset = make_quad_data(num_domains, num_points,train_coef)
    #num_domain, points, in_dim

    powers_f = np.stack([train_dataset[0] ** i for i in range(0, f_embed_dim + 1)],)
    # powers_g = np.stack([train_dataset [0]** i for i in range(0, g_embed_dim + 1)], dim=1)
    #power, num_domain, num_points, in_dim

    a=powers_f.squeeze().transpose()
    b=train_dataset[1].squeeze(axis=0)

    x=np.linalg.lstsq(a,b)
    print(x)

    #deg 6 error [7.56572675e-27]
    #deg 7, error [1.19885257e-05]
    #deg 8 [0.00114999]
    # result_f = self.invariant(powers_f)
    # result_g = self.train_variants[domain_num](powers_g)
    #
    # return self.eta(result_f, result_g)

