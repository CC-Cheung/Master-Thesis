# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J7bxCUtpGHCEwgT1_1m_lRL3c0rIo2t4
"""


import os
import torch
from torch import optim, nn, utils, Tensor
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor
import pytorch_lightning as pl
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
from pytorch_lightning import loggers as pl_loggers

def make_quad_data(num_domains, num_points):
  x=np.random.rand(num_domains, num_points, in_dim)
  coef=np.random.rand(num_domains,3)
  y=coef[:,:1, np.newaxis]*x**2 + coef[:,1:2, np.newaxis]*x + coef[:,-1:, np.newaxis]
  return torch.Tensor(np.concatenate((x,y) ,axis=-1)), torch.Tensor(coef)

class linear_relu(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, depth):
        super().__init__()
        self.net = nn.ModuleList([nn.Linear(in_dim, hidden_dim), nn.ReLU()])
        for i in range(depth):
            self.net.append(nn.Linear(hidden_dim, hidden_dim))
            self.net.append(nn.ReLU())

        self.net.append(nn.Linear(hidden_dim, out_dim))
        self.net=nn.Sequential(*self.net)
    def forward(self, x):
        return self.net(x)

class TransformerBlock(nn.Module):
    def __init__(self, in_dim, embed_dim, out_dim, num_heads=1):
        super().__init__()
        self.lin1 = linear_relu(in_dim, embed_dim, embed_dim, 2)

        self.att1 = nn.MultiheadAttention(embed_dim, num_heads)
        self.layer_norm1 = nn.LayerNorm(embed_dim)
        self.layer_norm2 = nn.LayerNorm(out_dim)

        self.lin2 = linear_relu(embed_dim, embed_dim, out_dim, 2)

    def forward(self, x):
        embed = self.lin1(x)
        embed = self.layer_norm1(self.att1(embed, embed, embed)[0] + embed)  # arbitrary
        result = self.layer_norm2(self.lin2(embed))
        return result

class BaselineTrans(pl.LightningModule):
    def __init__(self, in_dim, embed_dim, out_dim, num_heads=1):
        super().__init__()
        self.transformer = TransformerBlock(in_dim, embed_dim, out_dim, num_heads=num_heads)


        self.loss=nn.MSELoss()
    def forward(self, x):

        result = self.transformer(x)[:, 0]

        return result

    def training_step(self, batch, batch_idx):
        # training_step defines the train loop.
        # it is independent of forward

        x, y= batch
        result=self(x)
        loss = self.loss(y, result)
        # Logging to TensorBoard (if installed) by default
        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx) :
        x, y = batch
        result = self(x)
        loss = self.loss(y, result)
        # Logging to TensorBoard (if installed) by default
        self.log("val_loss", loss)

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=1e-3)
        return optimizer

if __name__=="__main__":
    # define any number of nn.Modules (or use your current ones)
    in_dim = 1
    hidden_dim_nature = 3
    hidden_dim_pred = 4
    embed_dim = 3
    num_heads = 1
    out_dim = 1
    depth = 2
    num_domains = 20
    train_coef = np.array([[0, 2, 3], [1, 0, 1], [1, 1, 0]])
    val_coef = np.array([[3, 2, 1]])
    num_points = 10
    num_val_io_pairs=5


    train_dataset = TensorDataset(*make_quad_data(num_domains, num_points))
    val_dataset = TensorDataset(*make_quad_data(1, num_points))

    train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)
    val_loader = DataLoader(val_dataset,batch_size=10)

    tb_logger = pl_loggers.TensorBoardLogger(save_dir="lightning_logs/test")

    base_trans=BaselineTrans(in_dim+out_dim,embed_dim, 3, num_heads)

    trainer1 = pl.Trainer(limit_train_batches=100,
                     logger=tb_logger,
                      max_epochs=100,
                      # log_every_n_steps = 5,
                    # accelerator="gpu", devices=1,
                      fast_dev_run=True
                      )
    # trainer2 = pl.Trainer(limit_train_batches=100,
    #                  logger=tb_logger,
    #                   max_epochs=2400,
    #                   log_every_n_steps = 1,
    # accelerator = "gpu", devices = 1,
    #
    # # fast_dev_run=True
    #
    #                   )

    # idea_module=Idea.load_from_checkpoint("/content/drive/MyDrive/Masters/Thesis/lightning_logs/version_5/checkpoints/epoch=1199-step=2400.ckpt")
    #
    # idea_module

    # trainer1.fit(idea_module2, train_loader,val_loader)

    trainer1.fit(base_trans, train_loader, val_loader)

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /content/drive/MyDrive/Masters/Thesis/lightning_logs --port=8008
#
# val_dataset[6]
#
# train_dataset[0]
#
# idea_module2(val_dataset[0:1][0],val_dataset[0:1][2] )

# val_dataset[0:1][1]



# Commented out IPython magic to ensure Python compatibility.
# %debug

# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J7bxCUtpGHCEwgT1_1m_lRL3c0rIo2t4
"""