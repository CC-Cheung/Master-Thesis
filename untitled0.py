# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J7bxCUtpGHCEwgT1_1m_lRL3c0rIo2t4
"""


import os
import torch
from torch import optim, nn, utils, Tensor
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor
import pytorch_lightning as pl
import numpy as np
from torch.utils.data import TensorDataset, DataLoader
from pytorch_lightning import loggers as pl_loggers

def POC_gen_points(num_points, num_io_pairs, in_dim, f, noise):
  x=np.arange(num_points)[:, np.newaxis]/20
  y=f(x)+noise(num_points, in_dim)
  concat=np.concatenate((x,y), axis=-1)
  concat_limited=concat[:num_io_pairs+1]

  return x, y, np.stack([np.delete(concat_limited, i%(num_io_pairs+1), axis=0)
                         for i in range(concat.shape[0])])
def make_POC_data(num_domain, num_points, num_io_pairs, in_dim,
                  # out_dim,
                  coef):
  temp_data=[POC_gen_points(num_points, num_io_pairs, in_dim,
                            # out_dim,
                            lambda x: coef[i,0]*x**2 + coef[i,1]*x + coef[i,2],
                            # lambda num_points, in_dim: np.random.rand(num_points, in_dim)) for i in range(coef.shape[0])
                            lambda num_points, in_dim: np.zeros((num_points, in_dim))) for i in range(num_domain)
             ]
  temp_data_grouped=list(zip(*temp_data))
  data=[torch.Tensor(np.concatenate(i, axis=0)) for i in temp_data_grouped]
  return data


class linear_relu(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim, depth):
        super().__init__()
        self.net = nn.ModuleList([nn.Linear(in_dim, hidden_dim), nn.ReLU()])
        for i in range(depth):
            self.net.append(nn.Linear(hidden_dim, hidden_dim))
            self.net.append(nn.ReLU())

        self.net.append(nn.Linear(hidden_dim, out_dim))
        self.net=nn.Sequential(*self.net)
    def forward(self, x):
        return self.net(x)


# define the LightningModule
# define the LightningModule
class Idea(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.nn_nature_lin = linear_relu(in_dim+out_dim, embed_dim, embed_dim, 1)
        self.nn_nature_att1= nn.MultiheadAttention(embed_dim, num_heads)
        self.layer_norm1=nn.LayerNorm(embed_dim)
        self.layer_norm2=nn.LayerNorm(embed_dim)

        self.nn_nature_lin2 = linear_relu(embed_dim, embed_dim, embed_dim, 1)


        self.nn_nature_att2= nn.MultiheadAttention(embed_dim, num_heads)

        self.nn_other = linear_relu(in_dim + embed_dim, hidden_dim_pred, out_dim, depth)
        self.loss=nn.MSELoss()
    def forward(self, x,io):
        io_temp=self.nn_nature_lin(io)
        embed = self.layer_norm1(self.nn_nature_att1(io_temp, io_temp, io_temp)[0]+io_temp)  # arbitrary
        embed = self.layer_norm2(self.nn_nature_lin2(embed))[:, 0]
        # embed=self.nn_nature_att2(embed, embed, embed)[0][:, 0]
        result = self.nn_other(torch.concat((x, embed), dim=1))
        return result

    def training_step(self, batch, batch_idx):
        # training_step defines the train loop.
        # it is independent of forward

        x, y, io = batch
        result=self(x,io)
        loss = self.loss(y, result)
        # Logging to TensorBoard (if installed) by default
        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx) :
        x, y, io = batch
        result=self(x,io)
        loss = self.loss(y, result)
        self.log("val_loss", loss)

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=1e-3)
        return optimizer
        
class Idea2(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.nn_nature_lin = linear_relu(in_dim+out_dim, embed_dim, embed_dim, 1)
        self.nn_nature_att1= nn.MultiheadAttention(embed_dim, num_heads)
        self.layer_norm1=nn.LayerNorm(embed_dim)
        self.layer_norm2=nn.LayerNorm(embed_dim)
        self.layer_norm3=nn.LayerNorm(embed_dim)

        self.nn_nature_lin2 = linear_relu(embed_dim, embed_dim, embed_dim, 1)


        self.nn_nature_att2= nn.MultiheadAttention(embed_dim, num_heads)

        self.nn_other = linear_relu(in_dim + embed_dim, hidden_dim_pred, out_dim, depth)
        self.loss=nn.MSELoss()
    def forward(self, x,io):
        io_temp=self.nn_nature_lin(io)
        embed = self.layer_norm1(self.nn_nature_att1(io_temp, io_temp, io_temp)[0]+io_temp)  # arbitrary
        embed=self.layer_norm2(self.nn_nature_att2(embed, embed, embed)[0]+embed)
        embed = self.layer_norm3(self.nn_nature_lin2(embed))[:, 0]
        result = self.nn_other(torch.concat((x, embed), dim=1))
        return result

    def training_step(self, batch, batch_idx):
        # training_step defines the train loop.
        # it is independent of forward

        x, y, io = batch
        result=self(x,io)
        loss = self.loss(y, result)
        # Logging to TensorBoard (if installed) by default
        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx) :
        x, y, io = batch
        result=self(x,io)
        loss = self.loss(y, result)
        self.log("val_loss", loss)

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=1e-3)
        return optimizer
class Baseline1(pl.LightningModule):
    def __init__(self):
        super().__init__()
        self.nn=linear_relu(1,10,1,3)        
        self.loss=nn.MSELoss()

    def forward(self, x):
        result = self.nn(x)
        return result
    def training_step(self, batch, batch_idx):
        # training_step defines the train loop.
        # it is independent of forward

        x, y, io = batch
        result=self(x)       
        loss = self.loss(y, result)
        # Logging to TensorBoard (if installed) by default
        self.log("train_loss", loss)
        return loss

    def validation_step(self, batch, batch_idx) :
        x, y, io = batch       
        result=self(x)       
        loss = self.loss(y, result)
        self.log("val_loss", loss)
        self.log("v_loss", loss, prog_bar=True)
    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=1e-3)
        return optimizer
if __name__=="__main__":
    # define any number of nn.Modules (or use your current ones)
    in_dim = 1
    hidden_dim_nature = 3
    hidden_dim_pred = 4
    embed_dim = 3
    num_heads = 1
    out_dim = 1
    depth = 2
    num_domain = 3
    in_range = 20
    train_coef = np.array([[0, 2, 3], [1, 0, 1], [1, 1, 0]])
    val_coef = np.array([[3, 2, 1]])
    num_points = 20
    num_val_io_pairs=5


    train_dataset = TensorDataset(*make_POC_data(num_domain, num_points,num_points, in_dim, train_coef))
    base_dataset=TensorDataset(*make_POC_data(1, num_val_io_pairs, num_val_io_pairs, in_dim,  val_coef))
    val_dataset = TensorDataset(*make_POC_data(1, num_points, num_val_io_pairs, in_dim, val_coef))

    train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)
    base_loader=DataLoader(base_dataset, batch_size=2, shuffle=True)
    val_loader = DataLoader(val_dataset,batch_size=10)

    tb_logger = pl_loggers.TensorBoardLogger(save_dir="")


    idea_module = Idea()
    baseline1_module=Baseline1()
    idea_module2 = Idea2()

    trainer1 = pl.Trainer(limit_train_batches=100,
                     logger=tb_logger,
                      max_epochs=2400,
                      log_every_n_steps = 5,
                    accelerator="gpu", devices=1,

                      # fast_dev_run=True
                      )
    trainer2 = pl.Trainer(limit_train_batches=100,
                     logger=tb_logger,
                      max_epochs=2400,
                      log_every_n_steps = 1,
    accelerator = "gpu", devices = 1,

    # fast_dev_run=True

                      )

    # idea_module=Idea.load_from_checkpoint("/content/drive/MyDrive/Masters/Thesis/lightning_logs/version_5/checkpoints/epoch=1199-step=2400.ckpt")
    #
    # idea_module

    # trainer1.fit(idea_module2, train_loader,val_loader)

    trainer2.fit(baseline1_module, base_loader, val_loader)

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /content/drive/MyDrive/Masters/Thesis/lightning_logs --port=8008
#
# val_dataset[6]
#
# train_dataset[0]
#
# idea_module2(val_dataset[0:1][0],val_dataset[0:1][2] )

# val_dataset[0:1][1]



# Commented out IPython magic to ensure Python compatibility.
# %debug

